# LLM-Tokenizer
## Introduction
My personal implementation of an LLM Tokenizer.

As of now I'm not entirely sure what the scope of this repo will be, I'll follow the lecture and will consider where to go next afterwards.

## Acknowledgements
This project was mainly inspired by the great video lectures on Neural Networks
by [Andrej Karpathy](https://karpathy.ai).

# Links and References
* [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
* [OpenAI Tokenizer](https://platform.openai.com/tokenizer)

# Bibliography
- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. _arXiv preprint arXiv:1906.05231_. https://doi.org/10.48550/arXiv.1906.05231
- Touvron, H., Martin, L., Stone, K., ... & Scialom, T. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. _arXiv preprint arXiv:2307.09288_.