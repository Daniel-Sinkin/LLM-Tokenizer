# LLM-Tokenizer
## Introduction
My personal implementation of an LLM Tokenizer.

As of now I'm not entirely sure what the scope of this repo will be, I'll follow the lecture and will consider where to go next afterwards.

Using Python version 3.11.6

## Acknowledgements
This project was mainly inspired by the great video lectures on Neural Networks
by [Andrej Karpathy](https://karpathy.ai).

# Links and References
* [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)
* [OpenAI Tokenizer](https://platform.openai.com/tokenizer)
* [TikTokenizer Webapp](https://tiktokenizer.vercel.app)
* [tinyshakespeare Data](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)
  * From the [char-rnn](https://github.com/karpathy/char-rnn) repo of Andrej Karpathy.
* [A Programmer's Introduction to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)
* [The Unicode Standard](https://www.unicode.org/versions/Unicode15.1.0/)
* [Python3: Unicode HOWTO](https://docs.python.org/3/howto/unicode.html)
* Wikipedia
  * [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)

# Bibliography
- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. _arXiv preprint arXiv:1906.05231_. https://doi.org/10.48550/arXiv.1906.05231
- Touvron, H., Martin, L., Stone, K., ... & Scialom, T. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. _arXiv preprint arXiv:2307.09288_.